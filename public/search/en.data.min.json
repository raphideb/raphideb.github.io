[{"id":0,"href":"/oracle/ash/","title":"Active Session History","parent":"Oracle","content":" ASH - Active Session History Table of content ASH - Active Session History Useful Tables and Views ASH Basics Description of the fields Useful Fields Checking available ASH data Saving ASH data How to write ASH scripts Basic Script Query from ASH Query from DASH Mapping to User and Object Name Limiting the time range Limiting the number of rows Making sense of the output Event Parameters Drilldown Finding blocking sessions Tracing a specific query PL/SQL queries Viewing specific events Other useful filters More ASH queries Track PGA usage Track TEMP usage The view **gv\\$active_session_history** (ASH) records in memory every second the wait events of all active sessions. It is the single most important source of information for deep performance analysis on a session or even query level, like what a session was doing in a particular moment in time, what queries it was executing or which other sessions were blocking it. For all the data it contains it is surprisingly easy to use. First I will explain how the ASH works and what fields it provides, then I show you a basic script and how to extend it with all sorts of queries to get the info that you need. Since the memory for ASH is limited, the data is only available for a few days or even hours. It is also usually gone when the instance was restarted, therefore, every 10 seconds a snapshot of the ASH data is stored permanently on disk and can be accessed through the view dba_hist_active_sess_history (DASH). The live view (Top activity) of Enterprise Manager is based on ASH data, AWR reports are based on dba_hist_* views.\nUseful Tables and Views To fully leverage all the information that the ASH provides, it helps to know some other useful views related to performance analysis.\nName Data Description Notes v$active_session_history Wait events for all sessions of a node in 1-second snapshots In memory, limited space gv$active_session_history Same as above, but for all nodes in a RAC cluster Has additional column \u0026ldquo;INST_ID\u0026rdquo; dba_hist_active_sess_history Historical 10-second snapshots of ASH Stored on disk; \u0026ldquo;INSTANCE_NUMBER\u0026rdquo; instead of \u0026ldquo;INST_ID\u0026rdquo; v$ash_info Statistics about ASH itself \u0026ldquo;OLDEST_SAMPLE_TIME\u0026rdquo; shows how far back data goes v$sql Stats for currently loaded SQL statements One row per child cursor dba_hist_sqltext Historical SQL text for SQL_IDs no longer in shared area Use set long 10000 to see full text dba_objects Info on all objects in the DB Useful for mapping to current_obj# in ASH dba_data_files Info on all data files Useful for mapping to current_file# in ASH ASH Basics All ASH/DASH data is stored at the CDB level and can be accessed either from the CDB or a specific PDB. However, when you want to map the USER_ID, CURRENT_OBJ#, CURRENT_FILE# etc to names stored in dba_views, it\u0026rsquo;s best to query ASH from within the corresponding PDB.\nA word about gv$active_session_history vs. v$active_session_history: While they are defacto the same on a non-RAC database, on a RAC database the v$ variant only shows sessions running on one instance, whereas gv$ shows all sessions an all instances of the database. I therefore made it a habbit to always use gv$active_session_history and include the inst_id in my queries. The same is true for many other v$ views, in RAC there\u0026rsquo;s usually a gv$ variant (like gv$session).\nNote that the dba_hist views are always cluster wide and store the data for all instances.\nDescription of the fields ASH and DASH have over 100 fields, and more are added with each release. The description of all fields is available here: https://docs.oracle.com/en/database/oracle/oracle-database/19/refrn/DBA_HIST_ACTIVE_SESS_HISTORY.html\nUseful Fields All fields of ASH/DASH can be useful in specific cases, but the following are the most commonly used:\nColumn Datatype Description INST_ID NUMBER Only in gv$active_session_history, RAC instance where the session is running INSTANCE_NUMBER NUMBER Only in dba_hist_active_sess_history, RAC instance where the session is running SAMPLE_ID NUMBER ID of the sample SAMPLE_TIME TIMESTAMP(3) Time at which the sample was taken SESSION_ID NUMBER Session identifier; maps to V$SESSION.SID SESSION_SERIAL# NUMBER Session serial number (uniquely identifies a session\u0026rsquo;s objects); maps to V$SESSION.SERIAL# SESSION_TYPE VARCHAR2(10) Session type: FOREGROUND/BACKGROUND USER_ID NUMBER Oracle user identifier; maps to V$SESSION.USER# SQL_ID VARCHAR2(13) SQL identifier of the SQL statement executed at the time of sampling SQL_OPNAME VARCHAR2(64) SQL command name (INSERT, DELETE, UPDATE, etc.) TOP_LEVEL_SQL_ID VARCHAR2(13) SQL identifier of the top-level SQL statement (for PL/SQL) SQL_PLAN_HASH_VALUE NUMBER Numeric representation of the SQL plan for the cursor EVENT VARCHAR2(64) If SESSION_STATE = WAITING, the event waited for; if ON CPU, this column is NULL P1TEXT VARCHAR2(64) Text of the first additional parameter P1 NUMBER First additional parameter P2TEXT VARCHAR2(64) Text of the second additional parameter P2 NUMBER Second additional parameter P3TEXT VARCHAR2(64) Text of the third additional parameter P3 NUMBER Third additional parameter WAIT_CLASS VARCHAR2(64) Wait class name of the event SESSION_STATE VARCHAR2(7) Session state: WAITING / ON CPU TIME_WAITED NUMBER If WAITING, time spent waiting for that event (in microseconds) BLOCKING_SESSION NUMBER Session ID of the blocking session BLOCKING_INST_ID NUMBER Instance number of the blocker CURRENT_OBJ# NUMBER Object ID referenced CURRENT_FILE# NUMBER File number referenced CURRENT_BLOCK# NUMBER Block ID referenced REMOTE_INSTANCE# NUMBER Remote instance serving the block PGA_ALLOCATED NUMBER PGA memory used (in bytes) TEMP_SPACE_ALLOCATED NUMBER TEMP memory used (in bytes) CON_DBID NUMBER Database ID of the pluggable database (PDB) PROGRAM VARCHAR2(48) Name of the operating system program MACHINE VARCHAR2(64) Client\u0026rsquo;s operating system machine name Good to Know:\nWait events that only affect the CDB, such as backup activities, are not shown when querying ASH in the PDB. A session in the PDB can have a blocking session from the CDB, e.g., the logwriter session. The wait events of the blocking session itself must then be analyzed via the CDB. Checking available ASH data Before analyzing a problem in the ASH, you should ensure that the data for the relevant period is still available:\nselect oldest_sample_time from gv$ash_info; -- output OLDEST_SAMPLE_TIME ------------------------------- 03-SEP-21 03.34.03.575000000 AM Query information before \u0026ldquo;OLDEST_SAMPLE_TIME\u0026rdquo; can only be viewed via DASH.\nSaving ASH data If you want to analyze the problem later, it is advisable to save the entire ASH to a table to not lose the data in it.\ncreate table ash_snapshot as select * from gv$active_session_history; In scripts, you should then select from this table instead:\n-- from gv$active_session_history from ash_snapshot Good to Know: Queries that are no longer in ASH and ran less than 10 seconds may not appear in DASH either. In such cases, the waits of a query can no longer be meaningfully analyzed.\nHow to write ASH scripts As you will shortly see, it\u0026rsquo;s very easy to start with a basic script to get an overview and then drill deeper to find the root cause of a particular problem. Even when you use scripts like Tanel Poder\u0026rsquo;s \u0026ldquo;ashtop\u0026rdquo; it\u0026rsquo;s good to have a basic understanding of how to query the ASH and get the maximum out of those scripts. I\u0026rsquo;ll also show the difference between queries from ASH or DASH.\nBasic Script Let\u0026rsquo;s start with a basic query which will be the blueprint for our more complex queries. It returns the most important columns of all wait events in the last hour for all nodes of a RAC database and works in both CDB and PDB.\nImportant: The \u0026ldquo;TIME_WAITED\u0026rdquo; column shows 0 for as long as the session was waiting for that particular event to end. When ordered by sample_time, the first non-zero value for a unique combination of inst_id/session_id/sql_id shows the total amount of time (in ms) the session had to wait on that event. When the \u0026ldquo;TIME_WAITED\u0026rdquo; and \u0026ldquo;EVENT\u0026rdquo; column are NULL (not 0), the session was on the CPU and not waiting for an event. You can select the session_state in your query if you want to confirm this.\nQuery from ASH set lines 400 set pages 100 column sample_time format a22 tru column event format a30 tru select sample_time, inst_id, sql_id, session_id, user_id, event, time_waited, current_obj#, blocking_session from gv$active_session_history where sample_time between sysdate-1/24 and sysdate order by sample_time; Query from DASH Difference from the ASH version:\ninstance_number instead of inst_id dba_hist_active_sess_history instead of gv$active_session_history set lines 400 set pages 100 column sample_time format a22 tru column event format a30 tru select sample_time, instance_number, sql_id, session_id, user_id, event, time_waited, current_obj#, blocking_session from dba_hist_active_sess_history where sample_time between sysdate-1/24 and sysdate order by sample_time; Sample Output:\nSAMPLE_TIME INST_ID SQL_ID SESSION_ID USER_ID EVENT TIME_WAITED CURRENT_OBJ# BLOCKING_SESSION 08-JUL-25 03.40.44.241 1 b913hx8hf360t 5059 89 direct path read 3764 45305959 08-JUL-25 03.41.07.142 2 di8yqa8tfsgqu 2312 89 cursor: pin S wait on X 234920 4463384 4801 08-JUL-25 03.41.07.142 2 cy6kjxcbcz8p3 893 89 db file parallel read 3275 48367311 08-JUL-25 03.41.07.142 2 di8yqa8tfsgqu 17769 89 cursor: pin S wait on X 274117 4463382 4801 Mapping to User and Object Name The user and object names can be joined to have names instead of ids, but only in the PDB:\nset lines 400 set pages 100 column sample_time format a22 tru column event format a30 tru column object_name format a30 select sample_time, inst_id, sql_id, session_id, b.username, event, time_waited, c.object_name, blocking_session from gv$active_session_history a join dba_users b on a.user_id = b.user_id join dba_objects c on a.current_obj# = c.object_id where sample_time between sysdate-1/24 and sysdate order by 1; Sample Output:\nSAMPLE_TIME INST_ID SQL_ID SESSION_ID USERNAME EVENT TIME_WAITED OBJECT_NAME BLOCKING_SESSION 08-JUL-25 03.40.54.854 2 84p8c9j3ctpua 14553 RAPHI_USER cursor: pin S wait on X 1191291 TRACKING_TABLE 11326 08-JUL-25 03.40.54.854 2 84p8c9j3ctpua 9272 RAPHI_USER cursor: pin S wait on X 1206884 TRK_PK_IDX 11326 08-JUL-25 03.42.08.156 3 cxfgr5c2j9zgg 7578 RAPHI_USER row cache lock 5642933 SUB_PART_MGR 4975 08-JUL-25 03.42.08.156 3 bz77a36407aty 4622 RAPHI_USER row cache lock 5650066 SUB_PART_MGR 4975 Now we are getting somewhere, we see the username and which objects where affected by the wait clearly.\nLimiting the time range The \u0026ldquo;sample_time\u0026rdquo; column can be used to limit the query period, allowing you to focus on the relevant part. This can be done relative to sysdate:\nwhere sample_time between sysdate-1/24 and sysdate or as a specific time range:\nwhere sample_time between to_date(\u0026#39;25-Nov-2020 00:40:00\u0026#39;, \u0026#39;DD-Mon-YYYY HH24:MI:SS\u0026#39;) and to_date(\u0026#39;25-Nov-2020 00:50:00\u0026#39;, \u0026#39;DD-Mon-YYYY HH24:MI:SS\u0026#39;) or both combined:\nwhere sample_time between to_date(\u0026#39;25-Nov-2020 00:40:00\u0026#39;, \u0026#39;DD-Mon-YYYY HH24:MI:SS\u0026#39;) and sysdate; Limiting the number of rows In one hour, ASH can record a huge number of rows, but often only the last 100 are of interest. To focus only on them, reverse the sort order and limit the output with fetch:\nwhere sample_time between sysdate-1/24 and sysdate order by sample_time desc fetch next 100 rows only Making sense of the output Event Parameters When searching for wait events, it\u0026rsquo;s often useful to select the \u0026ldquo;P1\u0026rdquo; column (and possibly P2, P3). These provide additional info about the wait event, e.g., for Row Cache Locks, which DC Cache is involved. The columns \u0026ldquo;P1TEXT\u0026rdquo; (and P2TEXT, P3TEXT) give a descriptive name to the value. To see what the individual P-values mean, you can look at v$event_name:\nset lines 400 col parameter1 format a20 col parameter2 format a20 col parameter3 format a20 select name, parameter1, parameter2, parameter3 FROM V$EVENT_NAME where name like \u0026#39;row%lock\u0026#39; order by name; Sample Output:\nNAME PARAMETER1 PARAMETER2 PARAMETER3 row cache lock cache id mode request You can then extend the basic query accordingly:\nset lines 400 set pages 100 column sample_time format a22 tru column event format a30 tru select sample_time, inst_id, sql_id, session_id, user_id, event, p1text, p1, time_waited from gv$active_session_history where sample_time between sysdate-1/24 and sysdate and event like \u0026#39;row%lock\u0026#39; order by 1; Drilldown With the basic script, you can get an overview of what was happening in the database during the selected period. The script can then be extended as needed to further narrow down the problem.\nFinding blocking sessions If, for example, a blocking_session is responsible for many waits, you can look at this session in more detail — the number in the \u0026ldquo;blocking_session\u0026rdquo; column is the session_id of the blocking session:\nwhere sample_time between sysdate-1/24 and sysdate and session_id = 123456 Tracing a specific query Similarly, if you want to look at a specific query, just filter by sql_id:\nwhere sample_time between sysdate-1/24 and sysdate and sql_id = \u0026#39;1ps650r41s7qr\u0026#39; PL/SQL queries In Oracle Enterprise Manager, you often only see the SQL_ID of the PL/SQL procedure among the top sessions. If you want to view all associated queries, filter by top_level_sql_id and specify the PL/SQL sql_id:\nwhere sample_time between sysdate-1/24 and sysdate and top_level_sql_id = \u0026#39;1ps650r41s7qr\u0026#39; Viewing specific events You can easily identify sessions affected by a specific wait using the Event column. A list of all possible wait events can be obtained with this query:\nselect name, wait_class FROM V$EVENT_NAME ORDER BY name; Alternatively, you can list only the waits that actually occur in ASH:\nselect distinct event from gv$active_session_history order by 1; To filter for specific event types, extend the basic query, for example:\nwhere sample_time between sysdate-1/24 and sysdate -- pick one of these: -- index contention and event = \u0026#39;enq: TX - index contention\u0026#39; -- all transaction contentions and event like \u0026#39;enq: TX%\u0026#39; -- read/write operations and (event like \u0026#39;db file%\u0026#39; or event like \u0026#39;direct path%\u0026#39;) -- all cluster waits (full RAC) and event like \u0026#39;gc%\u0026#39; Other useful filters Waits over 1 second: and time_waited \u0026gt; 1000000 Sessions accessing a specific object: and c.object_name = \u0026#39;NAME_OF_OBJECT\u0026#39; Sessions accessing a specific file: and current_file# = 134 Sessions accessing the same block in the same file (e.g., in case of contention): and current_file# = 134 and current_block# = 123456 More ASH queries Some other ASH queries which are not directly developed from the basic script above but can also be useful.\nTrack PGA usage With this query, you can check how much PGA each session_id has used:\nselect inst_id, session_id, session_serial#, max(pga_allocated)/1024/1024/1024 GB from gv$active_session_history where sample_time between sysdate-1/24 and sysdate group by inst_id,session_id, session_serial# order by 4; Track TEMP usage Similarly, you can find the sessions that have used the most TEMP:\nselect inst_id, session_id, session_serial#, max(temp_space_allocated)/1024/1024/1024 GB from gv$active_session_history where sample_time between sysdate-1/24 and sysdate group by inst_id, session_id, session_serial# order by 4; ","description":" ASH - Active Session History Table of content ASH - Active Session History Useful Tables and Views ASH Basics Description of the fields Useful Fields Checking available ASH data Saving ASH data How to write ASH scripts Basic Script Query from ASH Query from DASH Mapping to User and Object Name Limiting the time range Limiting the number of rows Making sense of the output Event Parameters Drilldown Finding blocking sessions Tracing a specific query PL/SQL queries Viewing specific events Other useful filters More ASH queries Track PGA usage Track TEMP usage The view **gv\\$active_session_history** (ASH) records in memory every second the wait events of all active sessions. It is the single most important source of information for deep performance analysis on a session or even query level, like what a session was doing in a particular moment in time, what queries it was executing or which other sessions were blocking it. For all the data it contains it is surprisingly easy to use. First I will explain how the ASH works and what fields it provides, then I show you a basic script and how to extend it with all sorts of queries to get the info that you need. Since the memory for ASH is limited, the data is only available for a few days or even hours. It is also usually gone when the instance was restarted, therefore, every 10 seconds a snapshot of the ASH data is stored permanently on disk and can be accessed through the view dba_hist_active_sess_history (DASH). The live view (Top activity) of Enterprise Manager is based on ASH data, AWR reports are based on dba_hist_* views.\n"},{"id":1,"href":"/","title":"Welcome to my little place on the internet","parent":"","content":"\nHi there! You found it, my little place on the internet that, powered by some wizardy, belongs to me but which I share with everybody. I use this space to post stuff I\u0026rsquo;ve picked up during the years, mainly PostgreSQL and Oracle related and some other things I have fun with.\nFeel free to contact me if you find any errors or need some clarification.\nOtherwise, grep a coffee and have fun ;)\n","description":"\nHi there! You found it, my little place on the internet that, powered by some wizardy, belongs to me but which I share with everybody. I use this space to post stuff I\u0026rsquo;ve picked up during the years, mainly PostgreSQL and Oracle related and some other things I have fun with.\nFeel free to contact me if you find any errors or need some clarification.\nOtherwise, grep a coffee and have fun ;)\n"},{"id":2,"href":"/oracle/ashtop/","title":"ashtop","parent":"Oracle","content":" 1. Introduction Tanel Poder\u0026rsquo;s script \u0026ldquo;ashtop.sql\u0026rdquo; greatly simplifies querying ASH and is essentially a fully parameterizable \u0026ldquo;Top Activity\u0026rdquo; view similar to what is known from OEM; you can also call it a \u0026ldquo;grep\u0026rdquo; for ASH.\nThe script comes in two variants:\n/nas/smdb/scripts/performance/sql/ash/ashtop.sql → Analysis of gv$active_session_history /nas/smdb/scripts/performance/sql/ash/dashtop.sql → Analysis of dba_hist_active_sess_history To use ashtop efficiently, it helps to know the structure of ASH, which is described here: ASH HowTo.\nThere\u0026rsquo;s also a very good video by Tanel on YouTube that explains how ashtop works: https://www.youtube.com/watch?v=D7bg1Am7Y9s\n2. Syntax The basic syntax is very simple:\n@ashtop \u0026lt;GROUP BY\u0026gt; \u0026lt;FILTER\u0026gt; \u0026lt;BEGIN_TIME\u0026gt; \u0026lt;END_TIME\u0026gt;; GROUP BY: What to group by, e.g., session_id, sql_id, event FILTER: What to filter by, e.g., session_type='FOREGROUND' or event like 'gc%' BEGIN_TIME: From which point in time, e.g., sysdate-1/24 END_TIME: Until which point in time, e.g., sysdate 2.1 Structure of ashtop The script is essentially structured as follows—the complete script has over 130 lines:\nSELECT \u0026amp;1 , COUNT(*) \u0026#34;TotalSeconds\u0026#34; FROM gv$active_session_history a , dba_users u , dba_objects o WHERE a.user_id = u.user_id (+) AND a.current_obj# = o.object_id(+) AND \u0026amp;2 AND sample_time BETWEEN \u0026amp;3 AND \u0026amp;4 GROUP BY \u0026amp;1 ORDER BY \u0026#34;TotalSeconds\u0026#34; DESC , \u0026amp;1 Good to know.\n2.2 Time Specifications BEGIN and END_TIME can be passed to the script in various ways; mixed forms are also possible:\nRelative to sysdate: @ashtop username,sql_id session_type=\u0026#39;FOREGROUND\u0026#39; sysdate-1/24/12 sysdate; ANSI syntax 1: @ashtop username,sql_id session_type=\u0026#39;FOREGROUND\u0026#39; DATE\u0026#39;2021-08-24\u0026#39; DATE\u0026#39;2021-08-25\u0026#39;; ANSI syntax 2: @ashtop username,sql_id session_type=\u0026#39;FOREGROUND\u0026#39; \u0026#34;TIMESTAMP\u0026#39;2024-04-17 18:00:00\u0026#39;\u0026#34; \u0026#34;TIMESTAMP\u0026#39;2024-04-17 20:00:00\u0026#39;\u0026#34;; to_date: @ashtop username,sql_id session_type=\u0026#39;FOREGROUND\u0026#39; \u0026#34;to_date(\u0026#39;24-AUG-21 18:00\u0026#39;, \u0026#39;DD-MON-YY HH24:MI\u0026#39;)\u0026#34; sysdate; More info on using ANSI syntax in Tanel\u0026rsquo;s scripts: https://tanelpoder.com/2012/12/29/a-tip-for-lazy-oracle-users-type-less-with-ansi-date-and-timestamp-sql-syntax/\n2.3 Derived Columns The script includes various derived columns that do not exist like this in ASH.\n2.3.1 username and objt Users and objects are only listed as IDs in ASH; if you want the names, you have to join dba_user and dba_objects. Ashtop simplifies this with the columns username and objt. These columns can be used as GROUP BY and as filters.\nFor example, to show the top 15 users accessing an object named \u0026ldquo;TABLE_NAME\u0026rdquo;:\n@ashtop username \u0026#34;objt=\u0026#39;TABLE_NAME\u0026#39;\u0026#34; sysdate-1/24 sysdate; Or the top objects accessed by the user \u0026ldquo;USER\u0026rdquo;:\n@ashtop objt \u0026#34;username=\u0026#39;USER\u0026#39;\u0026#34; sysdate-1/24 sysdate; The script always returns the top 15 rows that match the specified arguments. Usually, that\u0026rsquo;s enough, but it also means you don\u0026rsquo;t see the rest of the waits. If you can\u0026rsquo;t find the cause of a problem with ashtop, you should analyze the ASH with the basic script described under ASH HowTo. Both approaches are not mutually exclusive.\n2.3.2 event2 When a session is on the CPU, the EVENT column is NULL. To see that it was really on the CPU, you would have to output SESSION_STATE as well:\n@ashtop sql_id,session_state,event 1=1 sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_ID SESSION EVENT FIRST_SEEN LAST_SEEN Execs Seen 2343 .7 5% c0syzmwpwp3mw ON CPU 2021-09-10 12:44:19 2021-09-10 13:44:16 1449 1524 .4 3% fj40waf596jva ON CPU 2021-09-10 12:44:21 2021-09-10 13:44:16 7 To avoid always specifying session_state, you can simply use the column \u0026ldquo;event2\u0026rdquo;:\n@ashtop sql_id,event2 1=1 sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_ID EVENT2 FIRST_SEEN LAST_SEEN Execs Seen 2343 .7 5% c0syzmwpwp3mw ON CPU 2021-09-10 12:44:32 2021-09-10 13:44:29 1449 1528 .4 3% fj40waf596jva ON CPU 2021-09-10 12:44:30 2021-09-10 13:44:29 7 2.3.3 time_model_name ASH offers many columns that show which step of execution the query is currently in, e.g., Hard Parse or SQL Execution:\nColumn Description IN_CONNECTION_MGMT Was the session doing connection management at sampling? (Y/N) IN_PARSE Was the session parsing at sampling? (Y/N) IN_HARD_PARSE Was the session hard parsing at sampling? (Y/N) IN_SQL_EXECUTION Was the session executing SQL at sampling? (Y/N) IN_PLSQL_EXECUTION Was the session executing PL/SQL at sampling? (Y/N) IN_PLSQL_RPC Was the session executing inbound PL/SQL RPC at sampling? (Y/N) IN_PLSQL_COMPILATION Was the session compiling PL/SQL at sampling? (Y/N) IN_JAVA_EXECUTION Was the session executing Java at sampling? (Y/N) IN_BIND Was the session doing bind operations at sampling? (Y/N) A value of 1 for AAS and \u0026ldquo;ON CPU\u0026rdquo; EVENT2 means that approximately one CPU core was busy with this execution. The value scales; AAS of 5 means 5 cores were busy.\nColumn Description IN_CURSOR_CLOSE Was the session closing a cursor at sampling? (Y/N) IN_SEQUENCE_LOAD Was the session loading in sequence at sampling? (Y/N) IN_INMEMORY_QUERY Was the session querying In-Memory Column Store? (Y/N) IN_INMEMORY_POPULATE Was the session populating IM column store? (Y/N) IN_INMEMORY_PREPOPULATE Was the session prepopulating IM column store? (Y/N) IN_INMEMORY_REPOPULATE Was the session repopulating IM column store? (Y/N) IN_INMEMORY_TREPOPULATE Was the session trickle repopulating IM column store? (Y/N) IN_TABLESPACE_ENCRYPTION Was encryption/decryption of a tablespace occurring? (Y/N) To avoid passing all these columns as arguments to ashtop, you can use the derived column \u0026ldquo;time_model_name\u0026rdquo;. It is based on the ASH \u0026ldquo;Time_Model\u0026rdquo; bitfield and maps the value to a descriptive text:\n@ashtop sql_id,time_model_name 1=1 sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_ID TIME_MODEL_NAME FIRST_SEEN LAST_SEEN Execs Seen 737 .2 4% g9u23ytkurdwz SQL_EXECUTION 2021-09-13 12:18:58 2021-09-13 13:16:23 737 549 .2 3% CONNECTION_MGMT 2021-09-13 12:19:03 2021-09-13 13:18:27 1 480 .1 3% 1pyct56psc65z SQL_EXECUTION 2021-09-13 12:19:03 2021-09-13 13:18:51 480 478 .1 3% 6u8vh1hymdfh3 SQL_EXECUTION 2021-09-13 12:18:54 2021-09-13 13:18:52 478 Selection of Possible Time_Model_Name Time_Model_Name Meaning SQL_EXECUTION Statement is being executed PARSE Soft parse HARD_PARSE Hard parse PARSE HARD_PARSE Soft parse attempted, ended in hard parse CONNECTION_MGMT Login/Logouts BIND Binds are being filled (Empty line) Step not represented in ASH 2.4 Formatting Tanel assumes you use his scripts in a terminal where you can scroll horizontally. If not, you should at least increase the linesize and maybe shorten a few columns:\nset lines 400 col objt format a30 col event format a30 2.5 Customizing ashtop.sql Sometimes it is necessary to customize ashtop.sql, for example to show more than the top 15 events or to sort by another criterion. Always do this in your own copy:\ncp /nas/smdb/scripts/sql/ash/ashtop.sql ${HOME} cd ${HOME} vi ashtop.sql Change the order by:\nORDER BY -- TotalSeconds DESC time_waited DESC , \u0026amp;1 ... -- ROWNUM \u0026lt;= 15 ROWNUM \u0026lt;= 30 2.5.1 Copy of the ASH If a copy of the ASH has been created for backup, this must be adjusted in ashtop.sql. Always do this in your own copy of ashtop.sql:\ncp /nas/smdb/scripts/sql/ash/ashtop.sql ${HOME} cd ${HOME} vi ashtop.sql -- FROM gv$active_session_history a) a FROM ash_save220901 a) a 3. Output A simple query looks like this; the structure is always the same:\nsys@CIS_1/T1\u0026gt; @ashtop username,sql_id session_type=\u0026#39;FOREGROUND\u0026#39; sysdate-1/24 sysdate; Example output:\nSeconds AAS %This USERNAME SQL_ID FIRST_SEEN LAST_SEEN Execs Seen 1184 .2 9% CIS_ETL_APPL 37hs5hm167vhy 2019-11-26 12:35:38 2019-11-26 12:40:24 2 583 .1 4% CIS_TRANSFER_APPL bxj6rfn7du7c8 2019-11-26 11:57:30 2019-11-26 12:54:25 2 378 .1 3% CIS_TRANSFER_APPL 9jwmpa0zs1czv 2019-11-26 12:02:15 2019-11-26 12:03:23 1 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; First, these three columns are listed: Total Seconds: How many seconds all executions of the query are listed in ASH for the specified period AAS: Average Active Sessions spent per second with the wait event in the specified period %This: Ratio of the specified period to seconds all sessions spent with this event Then, the columns passed as the first argument (GROUP BY) are shown FIRST_SEEN: First execution in the specified period LAST_SEEN: Last execution in the specified period Distinct Execs Seen: Number of different executions in the specified period 4. Application 4.1 Example Queries There are countless ways to query ASH with ashtop; here is a selection:\n4.1.1 Top Events of the Last Hour @ashtop event2 1=1 sysdate-1/24 sysdate; 4.1.2 Top Waits of the Last 5 Minutes @ashtop event2 \u0026#34;event2 not like \u0026#39;ON CPU\u0026#39;\u0026#34; sysdate-1/24/12 sysdate; 4.1.3 Sessions with Highest CPU Usage @ashtop session_id,sql_id \u0026#34;event2=\u0026#39;ON CPU\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.4 Sessions with the Highest I/O Waits @ashtop session_id,sql_id,event2 \u0026#34;wait_class = \u0026#39;User I/O\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.5 What the Application is Doing @ashtop sql_id,sql_opname,event2 \u0026#34;username like \u0026#39;CIS%\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.6 What a Specific Session is Doing @ashtop sql_id,sql_opname,event2 \u0026#34;session_id=1667 and session_serial#=45570\u0026#34; sysdate-1/24 sysdate; 4.1.7 Which Waits on Which Objects @ashtop objt,event2,sql_id \u0026#34;event2 NOT LIKE \u0026#39;ON CPU\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.8 Which SQL Statement Reads the Longest from What @ashtop objt,sql_id \u0026#34;event like \u0026#39;%read%\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.9 What Causes the Most Log File Syncs @ashtop sql_opname,top_level_call_name \u0026#34;event=\u0026#39;log file sync\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.10 Are Top Sessions Blocked by Another @ashtop session_id,sql_id,event2,blocking_session session_type=\u0026#39;FOREGROUND\u0026#39; sysdate-1/24 sysdate; 4.1.11 Top Blocked Sessions @ashtop session_id,sql_id,event2,blocking_session \u0026#34;event2 NOT LIKE \u0026#39;ON CPU\u0026#39; and blocking_session is not null\u0026#34; sysdate-1/24 sysdate; 4.1.12 On Which Objects Sessions Block Each Other @ashtop username,sql_id,session_id,event,blocking_session,objt \u0026#34;event like \u0026#39;enq: TX%\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.1.13 Highest PGA Usage To do this, a private copy of ashtop.sql must be adjusted to sort by \u0026ldquo;PGA_ALLOCATED\u0026rdquo;:\nvi ashtop.sql ORDER BY -- TotalSeconds DESC PGA_ALLOCATED DESC Then:\n@ashtop pga_allocated,sql_id \u0026#34;session_type=\u0026#39;FOREGROUND\u0026#39;\u0026#34; sysdate-1/24 sysdate; 4.2 SQL Plan Statistics 4.2.1 With ashtop.sql Unless specified with a hint, SQL Monitor only records statements that ran at least 5 seconds. If you want to know which part of the execution plan consumed the most time for such a query, you have to use ASH. The following columns are available:\nColumn Description SQL_FULL_PLAN_HASH_VALUE Numerical representation of the complete SQL plan for the cursor SQL_PLAN_HASH_VALUE Numeric representation of the SQL plan for the cursor SQL_PLAN_LINE_ID SQL plan line ID SQL_PLAN_OPERATION Plan operation name SQL_PLAN_OPTIONS Plan operation options With the SQL_ID of the affected statement, you can see for short-lived queries where most time was spent:\n@ashtop sql_plan_line_id,sql_plan_operation,sql_plan_options \u0026#34;sql_id=\u0026#39;28w88hbtt3766\u0026#39;\u0026#34; sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_PLAN_LINE_ID SQL_PLAN_OPERATION SQL_PLAN_OPTIONS FIRST_SEEN LAST_SEEN Execs Seen 387 .1 58% 8 TABLE ACCESS BY LOCAL INDEX ROWID BATCHED 2021-09-13 10:38:46 2021-09-13 11:11:32 159 136 .0 20% 9 INDEX RANGE SCAN 2021-09-13 10:38:46 2021-09-13 11:11:31 114 108 .0 16% 15 TABLE ACCESS BY LOCAL INDEX ROWID BATCHED 2021-09-13 10:38:50 2021-09-13 11:11:33 108 27 .0 4% 16 INDEX SKIP SCAN 2021-09-13 10:38:50 2021-09-13 11:11:32 27 4 .0 1% 3 SORT UNIQUE 2021-09-13 10:38:49 2021-09-13 11:11:27 4 3 .0 0% 11 TABLE ACCESS BY GLOBAL INDEX ROWID BATCHED 2021-09-13 10:38:47 2021-09-13 10:38:53 3 2 .0 0% 12 INDEX RANGE SCAN 2021-09-13 10:38:48 2021-09-13 11:11:24 2 1 .0 0% 5 FILTER 2021-09-13 10:38:46 2021-09-13 10:38:46 1 This means that this query spends 58% of its time reading rows from a table, 20% searching the rows in the index, etc.\n4.2.2 With asqlmon.sql In the same directory as ashtop.sql is the script asqlmon.sql: /nas/smdb/scripts/performance/sql/ash/asqlmon.sql\nThe script simplifies querying plan statistics and provides even more information.\nFirst argument: sql_id Second argument: Plan Hash Value (or % for all plans) Third argument: Start time Fourth argument: End time Call for the same query as in the ashtop.sql example:\n@asqlmon 28w88hbtt3766 % sysdate-1/24 sysdate; 4.3 Empty SQL_ID Sometimes ashtop shows no SQL_ID for a session, either because the session belongs to a background process (in that case, the user is SYS and SESSION_TYPE is BACKGROUND) or because the session is mainly waiting for commits.\n@ashtop sql_id,event2,username 1=1 sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_ID EVENT2 USERNAME SESSION_TYPE FIRST_SEEN LAST_SEEN Execs Seen 3068 .9 57% log file sync BANCS_APPL FOREGROUND 2021-09-13 11:50:28 2021-09-13 12:50:23 1 809 .2 15% ON CPU BANCS_APPL FOREGROUND 2021-09-13 11:50:26 2021-09-13 12:50:21 1 100 .0 2% ON CPU SYS BACKGROUND 2021-09-13 11:51:19 2021-09-13 12:50:03 1 If you want to see what the user session is actually doing even though no SQL_ID is listed, you can search for \u0026ldquo;Top_Level_Call_Name\u0026rdquo;:\n@ashtop sql_id,top_level_call_name,event2 \u0026#34;sql_id is null and username=\u0026#39;BANCS_APPL\u0026#39;\u0026#34; sysdate-1/24 sysdate; Example output:\nSeconds AAS %This SQL_ID TOP_LEVEL_CALL_NAME EVENT2 FIRST_SEEN LAST_SEEN Execs Seen 2879 .8 65% COMMIT log file sync 2021-09-13 11:54:40 2021-09-13 12:54:38 1 195 .1 4% COMMIT ON CPU 2021-09-13 11:55:09 2021-09-13 12:54:13 1 183 .1 4% V8 Bundled Exec ON CPU 2021-09-13 11:55:56 2021-09-13 12:54:05 1 ⁂ ","description":" 1. Introduction Tanel Poder\u0026rsquo;s script \u0026ldquo;ashtop.sql\u0026rdquo; greatly simplifies querying ASH and is essentially a fully parameterizable \u0026ldquo;Top Activity\u0026rdquo; view similar to what is known from OEM; you can also call it a \u0026ldquo;grep\u0026rdquo; for ASH.\nThe script comes in two variants:\n/nas/smdb/scripts/performance/sql/ash/ashtop.sql → Analysis of gv$active_session_history /nas/smdb/scripts/performance/sql/ash/dashtop.sql → Analysis of dba_hist_active_sess_history To use ashtop efficiently, it helps to know the structure of ASH, which is described here: ASH HowTo.\n"},{"id":3,"href":"/oracle/","title":"Oracle","parent":"Welcome to my little place on the internet","content":"","description":""},{"id":4,"href":"/stuff/stuff/","title":"other stuff","parent":"Stuff","content":"some text\n","description":"some text\n"},{"id":5,"href":"/postgres/pgbench/","title":"pgbench","parent":"Postgres","content":" The PostgreSQL benchmark utility Table of content The PostgreSQL benchmark utility Installation Initial Setup Tables Installation in dedicated database Run the benchmark Sample output Create your own benchmark Write the script Run your script Compare TPC-B vs TPC-DS like execution TPC-B like TPC-DS like Conclusion Including a benchmark utility with a database product demonstrates a strong commitment to performance and transparency. For PostgreSQL, the built-in tool is pgbench, which allows users to test various aspects of the database system. Not only that, it can also run benchmarks on any set of tables with any script that you created yourself, making it the perfect tool to benchmark a specific application workload. The official documentation is here: https://www.postgresql.org/docs/current/pgbench.html\nInstallation In most cases, pgbench is installed in your system’s binary directory when you install PostgreSQL, typically /usr/bin or in the PostgreSQL bindir:\nls $(pg_config --bindir)/pgbench # output /usr/lib/postgresql/17/bin/pgbench By default, pgbench simulates a workload similar to the TPC-B benchmark, generating many small transactions involving single-row updates, lookups, and inserts. However, it’s straightforward to design custom benchmarks, as described further below.\nInitial Setup Before using pgbench, the necessary tables must be set up in the database. You need to specify a scale factor (-s) during this process, the higher the scale factor, the larger the tables will be. For example, with a scale factor of 100, the pgbench_accounts table will be created with 10 million rows.\nTables The following tables will be dropped and recreated if they already exist:\npgbench_accounts pgbench_branches pgbench_history pgbench_tellers If you do not specify a database, the tables will be created in the database that psql connects you to (typically your OS user, e.g. postgres). To create the tables, run:\npgbench -i -s 100 Installation in dedicated database If you want the tables to be created in their own database, you need to create a role, database and schema first:\npsql -- create a role: CREATE ROLE pgbench WITH LOGIN; -- create the database owned by that role CREATE DATABASE pgbench OWNER pgbench; -- connect to the newly created schema \\c pgbench CREATE SCHEMA pgbench; -- grant the necessary rights GRANT ALL ON SCHEMA pgbench TO pgbench; exit; Now you are ready to install pgbench into the newly created database:\npgbench -i -s 100 -d pgbench -U pgbench Run the benchmark The most important parameters are:\n-j [number of jobs] -c [number of clients] -T [time to run in seconds] To test the database without being influenced by other factors like cpu scheduling, I prefer to have the same number of jobs and clients and both do not exceed the count of half of my machine\u0026rsquo;s CPU cores.\nTPC-B like example to run the benchmark for 10 minutes:\npgbench -c 20 -j 20 -T 600 -- or in a dedicated database pgbench -c 20 -j 20 -T 600 -d pgbench -U pgbench From here on I assume that the tables are created in the pgbench database and schema.\nSample output The most meaningful numbers to compare between runs are the tps (transactions per second) and the number of transactions actually processed:\npostgres@plexus:~$ pgbench -c 20 -j 20 -T 60 -d pgbench -U pgbench pgbench (17.5 (Debian 17.5-1.pgdg120+1)) starting vacuum...end. transaction type: \u0026lt;builtin: TPC-B (sort of)\u0026gt; scaling factor: 100 query mode: simple number of clients: 20 number of threads: 20 maximum number of tries: 1 duration: 60 s number of transactions actually processed: 318422 number of failed transactions: 0 (0.000%) latency average = 3.768 ms initial connection time = 9.239 ms tps = 5307.477304 (without initial connection time) Create your own benchmark In this example, I am creating a benchmark that more closely resembles TPC-DS, which is designed for decision support systems. While it is by no means a true TPC-DS benchmark which consists of multiple databases and star schemasTPC-DS workloads typically involve large-scale updates, inserts, and complex queries that operate on many rows — characteristic of typical data warehouse environments.\nIn general, while the default TPC-B benchmark primarily tests CPU/memory performance and random access I/O through many small transactions, a TPC-DS-like benchmark tends to generate more sequential I/O due to larger table scans and aggregations, often hitting the shared buffer better.\nWrite the script While it is possible to create your own tables and fill them with data, I am going to use the tables which I already created previously with \u0026ldquo;pgbench -i\u0026rdquo;. Due to the underlying data, this script is by no means a true TPC-DS benchmark, which usually consists of multiple tables in a star schema. But it should you give an idea how to write a true TPC-DS benchmark your self.\nCopy/paste the following lines into a file, let\u0026rsquo;s call it \u0026ldquo;dwhbench.sql\u0026rdquo;:\nBEGIN; -- 1. Total account balance per branch (roll-up by branch) SELECT b.bid, SUM(a.abalance) AS total_balance FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid GROUP BY b.bid; -- 2. Average, min, max account balance per branch (analytics) SELECT b.bid, AVG(a.abalance) AS avg_balance, MIN(a.abalance) AS min_balance, MAX(a.abalance) AS max_balance FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid GROUP BY b.bid; -- 3. Top 10 branches by total account balance (reporting) SELECT b.bid, SUM(a.abalance) AS total_balance FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid GROUP BY b.bid ORDER BY total_balance DESC LIMIT 10; -- 4. Number of accounts per branch with balance above a threshold (filtering) SELECT b.bid, COUNT(a.aid) AS num_high_value_accounts FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid WHERE a.abalance \u0026gt; 100000 GROUP BY b.bid ORDER BY num_high_value_accounts DESC LIMIT 10; -- 5. Teller performance: total transactions and sum of deltas per teller SELECT t.tid, COUNT(h.aid) AS num_transactions, SUM(h.delta) AS total_delta FROM pgbench_tellers t LEFT JOIN pgbench_history h ON t.tid = h.tid GROUP BY t.tid ORDER BY total_delta DESC LIMIT 10; -- 6. Daily transaction summary for the last 7 days (time-based reporting) SELECT DATE(h.mtime) AS day, COUNT(*) AS num_transactions, SUM(h.delta) AS total_delta FROM pgbench_history h WHERE h.mtime \u0026gt;= CURRENT_DATE - INTERVAL \u0026#39;7 days\u0026#39; GROUP BY day ORDER BY day DESC; -- 7. Branches with most active tellers (multi-level aggregation) SELECT b.bid, COUNT(DISTINCT t.tid) AS num_tellers, COUNT(h.aid) AS num_transactions FROM pgbench_branches b JOIN pgbench_tellers t ON b.bid = t.bid LEFT JOIN pgbench_history h ON t.tid = h.tid GROUP BY b.bid ORDER BY num_transactions DESC LIMIT 10; -- 8. Insert a new transaction (history) with valid keys INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ( (SELECT tid FROM pgbench_tellers ORDER BY random() LIMIT 1), (SELECT bid FROM pgbench_branches ORDER BY random() LIMIT 1), (SELECT aid FROM pgbench_accounts ORDER BY random() LIMIT 1), (random() * 1000 - 500)::INTEGER, now() ); COMMIT; You can modify the script to your liking if needed or install the pgbench tables with a higher scale factor than 100 (-s 1000).\nRun your script Now execute the script with pgbench:\npgbench -c 20 -j 20 -T 60 -d pgbench -U pgbench -f dwhbench.sql Sample Output - note that the tps is significantelly lower than in the TPC-B like runs before:\npostgres@plexus:~$ pgbench -c 20 -j 20 -T 60 -d pgbench -U pgbench -f dwhbench.sql pgbench (17.5 (Debian 17.5-1.pgdg120+1)) starting vacuum...end. transaction type: dwhbench.sql scaling factor: 1 query mode: simple number of clients: 20 number of threads: 20 maximum number of tries: 1 duration: 60 s number of transactions actually processed: 158 number of failed transactions: 0 (0.000%) latency average = 8223.323 ms initial connection time = 7.292 ms tps = 2.432107 (without initial connection time) Compare TPC-B vs TPC-DS like execution With the pgstat_snap extension I can now easily compare the execution of both benchmarks, the default pgbench benchmark and the dwhbench I created before. If you don\u0026rsquo;t have the extension installed, you can download it here: https://github.com/raphideb/pgstat_snap\nFor the tests, I first reset all pg_stat* and psgstat_snap* statistics:\npsql select pgstat_snap_reset(2); Then I start collecting snapshots in one terminal window for 70 seconds:\ncall pgstat_snap_collect(1,70); and immediately after start the benchmark in another terminal:\n# for default TPC-B like pgbench -c 20 -j 20 -T 60 -d pgbench -U pgbench # for our custom TPC-DS like pgbench -c 20 -j 20 -T 60 -d pgbench -U pgbench -f dwhbench.sql TPC-B like The pgbench default benchmark executes statements many times per second but usually only affects one row per execution, as shown by the number of rows affected each second divided by the number of times the statement was called (rows_d/called_d). The number of blocks dirtied in the shared buffer is also often quite high, indicative of a lot of reads from the filesystem.\nselect * from pgstat_snap_diff order by 1; snapshot_time | queryid | query | datname | usename | wait_event_type | wait_event | rows_d | calls_d | exec_ms_d | sb_hit_d | sb_read_d | sb_dirt_d | sb_write_d ---------------------+----------------------+---------------------+---------+---------+-----------------+---------------+--------+---------+--------------+----------+-----------+-----------+------------ 2025-07-06 11:56:10 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 2646 | 2646 | 5515.640933 | 13334 | 4806 | 2988 | 8 2025-07-06 11:56:11 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | Lock | transactionid | 2630 | 2630 | 834.971431 | 11784 | 0 | 0 | 0 2025-07-06 11:56:11 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 2626 | 2626 | 5365.836472 | 13598 | 4594 | 2936 | 239 2025-07-06 11:56:12 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | Lock | transactionid | 2614 | 2614 | 783.287074 | 11692 | 0 | 0 | 0 2025-07-06 11:56:12 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 2619 | 2619 | 5390.995660 | 13498 | 4521 | 2898 | 531 2025-07-06 11:56:13 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | Lock | transactionid | 2570 | 2570 | 907.785515 | 11650 | 0 | 0 | 0 2025-07-06 11:56:13 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 2568 | 2568 | 5206.180391 | 13230 | 4336 | 2796 | 1112 2025-07-06 11:56:14 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 3111 | 3111 | 5063.160774 | 15935 | 5181 | 3356 | 2626 2025-07-06 11:56:14 | 6874825970108732267 | INSERT INTO pgbench | pgbench | pgbench | | | 10929 | 10929 | 73.726408 | 11169 | 0 | 69 | 86 2025-07-06 11:56:17 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | Lock | transactionid | 10880 | 10880 | 2454.568335 | 48787 | 0 | 0 | 0 2025-07-06 11:56:17 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 7770 | 7770 | 9218.660031 | 39796 | 12937 | 8368 | 7342 2025-07-06 11:56:17 | 6874825970108732267 | INSERT INTO pgbench | pgbench | pgbench | | | 7766 | 7766 | 51.563130 | 8122 | 0 | 41 | 65 2025-07-06 11:56:18 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | | | 4067 | 4067 | 903.844398 | 18373 | 0 | 0 | 0 2025-07-06 11:56:18 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 4061 | 4061 | 4356.176667 | 20545 | 6727 | 4308 | 3985 2025-07-06 11:56:18 | 7882337555304992192 | UPDATE pgbench_tell | pgbench | pgbench | | | 22763 | 22763 | 947.264902 | 115032 | 0 | 0 | 0 2025-07-06 11:56:18 | 6874825970108732267 | INSERT INTO pgbench | pgbench | pgbench | Client | ClientRead | 4067 | 4067 | 26.740198 | 4240 | 0 | 31 | 53 2025-07-06 11:56:19 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | | | 4087 | 4087 | 894.378344 | 18372 | 0 | 0 | 0 2025-07-06 11:56:19 | 7882337555304992192 | UPDATE pgbench_tell | pgbench | pgbench | | | 4085 | 4085 | 176.151355 | 20660 | 0 | 0 | 0 2025-07-06 11:56:19 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | IO | DataFileRead | 4088 | 4088 | 4107.538425 | 20958 | 6794 | 4407 | 3877 2025-07-06 11:56:20 | -3717316371818285038 | UPDATE pgbench_acco | pgbench | pgbench | | | 4162 | 4162 | 4032.420140 | 21028 | 6905 | 4433 | 3993 2025-07-06 11:56:20 | -9070982371082842952 | UPDATE pgbench_bran | pgbench | pgbench | Client | ClientRead | 4158 | 4158 | 943.804925 | 18834 | 0 | 0 | 0 2025-07-06 11:56:20 | 6874825970108732267 | INSERT INTO pgbench | pgbench | pgbench | | | 8245 | 8245 | 53.479212 | 8369 | 0 | 54 | 85 TPC-DS like This benchmark tells a different story, some queries return one row per call, others 10 or 100 per call (rows_d/calls_d). Also note that the number of blocks dirtied and evicted from the buffer is near 0 (sb_dirt_d, sb_write_d) whereas in the TPC-B, blocks were constantly unloaded and loaded into the shared buffer.\nselect * from pgstat_snap_diff order by 1; snapshot_time | queryid | query | datname | usename | wait_event_type | wait_event | rows_d | calls_d | exec_ms_d | sb_hit_d | sb_read_d | sb_dirt_d | sb_write_d ---------------------+----------------------+---------------------+---------+---------+-----------------+---------------+--------+---------+--------------+----------+-----------+-----------+------------ 2025-07-06 12:29:00 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | LWLock | BufferMapping | 300 | 3 | 2090.854209 | 42644 | 457201 | 0 | 0 2025-07-06 12:29:01 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 500 | 5 | 5575.932052 | 58345 | 774700 | 0 | 0 2025-07-06 12:29:01 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 80 | 8 | 14012.869241 | 108454 | 1224336 | 0 | 0 2025-07-06 12:29:02 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 200 | 2 | 2712.562823 | 41057 | 292153 | 0 | 0 2025-07-06 12:29:02 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | IO | DataFileRead | 20 | 2 | 1752.724632 | 31600 | 301620 | 0 | 0 2025-07-06 12:29:02 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 4 | 4 | 11356.669967 | 2788994 | 469954 | 3 | 18 2025-07-06 12:29:03 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 200 | 2 | 1537.711768 | 27657 | 305573 | 0 | 0 2025-07-06 12:29:03 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 100 | 1 | 695.818981 | 9872 | 156743 | 0 | 0 2025-07-06 12:29:03 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 3 | 3 | 8762.063660 | 2049418 | 394793 | 2 | 5 2025-07-06 12:29:04 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | LWLock | BufferMapping | 300 | 3 | 3760.906149 | 54054 | 445761 | 0 | 0 2025-07-06 12:29:04 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 100 | 1 | 751.276443 | 15988 | 150627 | 0 | 0 2025-07-06 12:29:04 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 3 | 3 | 8790.948263 | 1885512 | 558699 | 3 | 12 2025-07-06 12:29:04 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 40 | 4 | 4158.021389 | 60654 | 605786 | 0 | 0 2025-07-06 12:29:05 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 300 | 3 | 6126.329681 | 34905 | 464880 | 0 | 0 2025-07-06 12:29:05 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 7 | 7 | 20024.136128 | 4948620 | 754539 | 3 | 23 2025-07-06 12:29:05 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 300 | 3 | 2650.404371 | 41028 | 458807 | 0 | 0 2025-07-06 12:29:06 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 30 | 3 | 2509.879189 | 50945 | 448890 | 0 | 0 2025-07-06 12:29:06 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | LWLock | BufferMapping | 2 | 2 | 5933.317393 | 1257252 | 372222 | 2 | 14 2025-07-06 12:29:06 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 200 | 2 | 2818.259800 | 28190 | 305020 | 0 | 0 2025-07-06 12:29:06 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 100 | 1 | 775.519352 | 14036 | 152579 | 0 | 0 2025-07-06 12:29:07 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 800 | 8 | 15163.238847 | 129899 | 1202881 | 0 | 0 2025-07-06 12:29:07 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | IO | DataFileRead | 400 | 4 | 7343.899760 | 42480 | 623920 | 0 | 0 2025-07-06 12:29:07 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 2 | 2 | 5753.122885 | 1257700 | 371774 | 2 | 13 2025-07-06 12:29:07 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 10 | 1 | 697.975914 | 15494 | 151121 | 0 | 0 2025-07-06 12:29:08 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | IO | DataFileRead | 200 | 2 | 1544.049097 | 25949 | 307281 | 0 | 0 2025-07-06 12:29:08 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 100 | 1 | 1979.149370 | 25598 | 140997 | 0 | 1 2025-07-06 12:29:08 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 1 | 1 | 3134.628218 | 629162 | 185575 | 1 | 6 2025-07-06 12:29:08 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 20 | 2 | 1773.277487 | 25037 | 308183 | 0 | 0 2025-07-06 12:29:09 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 700 | 7 | 14010.989075 | 70769 | 1095416 | 0 | 0 2025-07-06 12:29:09 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 40 | 4 | 5428.666678 | 53151 | 613269 | 0 | 0 2025-07-06 12:29:09 | -6153627433925560805 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 200 | 2 | 3986.136102 | 36939 | 296251 | 0 | 0 2025-07-06 12:29:09 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 3 | 3 | 8991.218544 | 2000799 | 443412 | 2 | 12 2025-07-06 12:29:10 | 6510738175259849580 | SELECT b.bid, SUM(a | pgbench | pgbench | | | 30 | 3 | 2466.541453 | 41033 | 458802 | 0 | 0 2025-07-06 12:29:10 | 4503905990032662730 | SELECT b.bid, | pgbench | pgbench | | | 100 | 1 | 2159.522420 | 12549 | 154046 | 0 | 0 2025-07-06 12:29:10 | 3571751447725039568 | INSERT INTO pgbench | pgbench | pgbench | | | 1 | 1 | 2846.755759 | 628987 | 185750 | 1 | 10 Conclusion pgbench is a very powerful tool to benchmark a PostgreSQL database however you want. It\u0026rsquo;s very easy to create your own application workload and compare it across different systems or before/after an upgrade, especially when combined with the pgstat_snap extension.\nhappy benchmarking ;)\n","description":" The PostgreSQL benchmark utility Table of content The PostgreSQL benchmark utility Installation Initial Setup Tables Installation in dedicated database Run the benchmark Sample output Create your own benchmark Write the script Run your script Compare TPC-B vs TPC-DS like execution TPC-B like TPC-DS like Conclusion Including a benchmark utility with a database product demonstrates a strong commitment to performance and transparency. For PostgreSQL, the built-in tool is pgbench, which allows users to test various aspects of the database system. Not only that, it can also run benchmarks on any set of tables with any script that you created yourself, making it the perfect tool to benchmark a specific application workload. The official documentation is here: https://www.postgresql.org/docs/current/pgbench.html\n"},{"id":6,"href":"/postgres/pgstat_snap/","title":"pgstat_snap","parent":"Postgres","content":" Purpose of this script The cumulative statistics system (CSS) in PostgreSQL and pg_stat_statements in particular lack any timing information, all values are cumulative and the only way to figure out the difference between query executions is to reset the stats every time or work with averages.\nWith the pgstat_snap extension, you can create timestamped snapshots of pg_stat_statements and pg_stat_activity when needed. It also provides views that show the difference between every snapshot for every query and database.\nRequirements pg_stat_statements must be loaded and tracking activated in the postgres config:\nshared_preload_libraries = \u0026#39;pg_stat_statements\u0026#39; Recommended settings:\npg_stat_statements.track = all pg_stat_statements.track_utility = off The extension has to be created in the database in which pgstat_snap will be installed:\ncreate extension pg_stat_statements; Installation To install the extension, download these files:\npgstat_snap--1.0.sql pgstat_snap.control And copy them to the extension directory of PostgreSQL\nsudo cp pgstat_snap* $(pg_config --sharedir)/extension/ You can then install the extension in any database that has the pg_stat_statements extension enabled, superuser right are NOT needed:\ncreate extension pgstat_snap; It can also be installed into a different schema but be sure to have it included in the search_path:\ncreate extension pgstat_snap schema my_schema; This will create the following tables and views:\npgstat_snap_stat_history -\u0026gt; pg_stat_statements history (complete snapshot) pgstat_snap_act_history -\u0026gt; pg_stat_activity history (complete snapshot) pgstat_snap_diff_all -\u0026gt; view containing the sum and difference of each statement between snapshots pgstat_snap_diff -\u0026gt; view containing only the difference of each statement between snapshots Usage Start gathering snapshots with, e.g. every 1 second 60 times:\nCALL pgstat_snap_collect(1, 60); Or gather a snapshot every 5 seconds for 10 minutes:\nCALL pgstat_snap_collect(5, 120); IMPORTANT: on very busy clusters with many databases a lot of data can be collected, 500mb per minute or more. Don\u0026rsquo;t let it run for a very long time with short intervals, unless you have the disk space for it.\nReset Because everything is timestamped, a reset is usually not needed between CALLs to create_snapshot. But you can to cleanup and keep the tables smaller. You can also reset pg_stats*.\nReset all pgstat_snap tables with:\nSELECT pgstat_snap_reset(); -\u0026gt; reset only pgstat_snap.pgstat*history tables SELECT pgstat_snap_reset(1); -\u0026gt; also select pg_stat_statements_reset() SELECT pgstat_snap_reset(2); -\u0026gt; also select pg_stat_reset() How it works The first argument to create_snapshot is the interval in seconds, the second argument is how many snapshots should be collected. Every interval seconds, select * from pg_stat_statements will be inserted into pgstat_snap_stat_history and select * from pgstat_act_statements into pgstat_snap_act_history.\nFor every row, a timestamp will be added. Only rows where the \u0026ldquo;rows\u0026rdquo; column has changed will be inserted into pgstat_snap_stat_history and always only one row per timestamp, dbid and queryid. Every insert is immediately committed to be able to live follow the tables/views.\nThe views have a _d column which displays the difference between the current row and the last row where the query was recorded in the pgstat_snap_stat_history table. NULL values in rows_d, calls_d and so on mean, that no previous row for this query was found because it was executed the first time since create_snapshot was running.\nThe views also contain the datname, wait events and the first 20 characters of the query, making it easier to identify queries of interest.\nUninstall To completely uninstall pgstat_snap, run:\nDROP EXTENSION pgstat_snap; Views description pgstat_snap_diff This view only contains the difference between the previous and next execution of a queryid/dbid pair:\nColumn Name Description snapshot_time Timestamp queryid Query ID query Query Text (first 20 characters) datname Database Name usename Username wait_event_type Event Type - NULL if no wait occurred wait_event Wait Event - NULL if no wait occurred rows_d Difference in rows from the previous snapshot_time calls_d Difference in calls from the previous snapshot_time exec_ms_d Difference in total execution time from the previous snapshot_time sb_hit_d Difference in shared block hits from the previous snapshot_time sb_read_d Difference in shared block reads from the previous snapshot_time sb_dirt_d Difference in shared blocks dirtied from the previous snapshot_time sb_write_d Difference in shared blocks written from the previous snapshot_time Sample output select * from pgstat_snap_diff order by 1; snapshot_time queryid query datname usename wait_event_type wait_event rows_d calls_d exec_ms_d sb_hit_d sb_read_d sb_dirt_d sb_write_d 2025-03-25 11:00:19 4380144606300689468 UPDATE pgbench_tell postgres postgres Lock transactionid 4485 4485 986.262098 22827 0 0 0 2025-03-25 11:00:20 4380144606300689468 UPDATE pgbench_tell postgres postgres Lock transactionid 1204 1204 228.822413 6115 0 0 0 2025-03-25 11:00:20 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1204 1204 1758.190499 5655 0 0 0 2025-03-25 11:00:21 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1273 1273 2009.227575 6024 0 0 0 2025-03-25 11:00:22 2931033680287349001 UPDATE pgbench_acco postgres postgres Client ClientRead 9377 9377 1818.464415 66121 3699 7358 35 2025-03-25 11:00:22 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1356 1356 1659.806856 6341 0 0 0 2025-03-25 11:00:23 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1168 1168 1697.322874 5484 0 0 0 2025-03-25 11:00:24 7073332947325598809 UPDATE pgbench_bran postgres postgres [NULL] [NULL] 1135 1135 1539.999618 5237 0 0 0 2025-03-25 11:00:24 5744520630148654507 SELECT abalance FROM postgres postgres [NULL] [NULL] 11679 11679 114.347514 49451 0 0 0 2025-03-25 11:00:25 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1274 1274 1861.747733 6043 0 0 0 2025-03-25 11:00:26 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1080 1080 1855.803660 5080 0 0 0 2025-03-25 11:00:27 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1155 1155 1669.499608 5373 0 0 0 2025-03-25 11:00:27 7113545590461720994 SELECT CASE WHEN pg postgres postgres Client ClientRead 1 1 0.098783 0 0 0 0 2025-03-25 11:00:28 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 1178 1178 1623.365492 5466 0 0 0 2025-03-25 11:00:29 2931033680287349001 UPDATE pgbench_acco postgres postgres Client ClientRead 8350 8350 1456.806356 58400 3317 6135 30 pgstat_snap_diff_all This view contains the difference between the previous and next execution of a queryid/dbid pair and the sum of the fields as recorded in pg_stat_statements at that time:\nColumn Name Description snapshot_time Timestamp queryid Query ID query Query Text (first 20 characters) datname Database Name usename Username wait_event_type Event Type - NULL if no wait occurred wait_event Wait Event - NULL if no wait occurred rows Value of rows at this time in pg_stat_statements rows_d Difference in rows from the previous snapshot_time calls Value of calls at this time in pg_stat_statements calls_d Difference in calls from the previous snapshot_time exec_ms Value of total execution time at this time in pg_stat_statements exec_ms_d Difference in total execution time from the previous snapshot_time sb_hit Value of shared block hits at this time in pg_stat_statements sb_hit_d Difference in shared block hits from the previous snapshot_time sb_read Value of shared block reads at this time in pg_stat_statements sb_read_d Difference in shared block reads from the previous snapshot_time sb_dirt Value of shared blocks dirtied at this time in pg_stat_statements sb_dirt_d Difference in shared blocks dirtied from the previous snapshot_time sb_write Value of shared blocks written at this time in pg_stat_statements sb_write_d Difference in shared blocks written from the previous snapshot_time Sample output select * from pgstat_snap_diff_all order by 1; snapshot_time queryid query datname usename wait_event_type wait_event rows rows_d calls calls_d exec_ms exec_ms_d sb_hit sb_hit_d sb_read sb_read_d sb_dirt sb_dirt_d sb_write sb_write_d 2025-03-25 11:00:19 4380144606300689468 UPDATE pgbench_tell postgres postgres Lock transactionid 241818 4485 241818 4485 39693.679945 986.262098 1237102 1237098 4 0 90 0 22 0 2025-03-25 11:00:20 4380144606300689468 UPDATE pgbench_tell postgres postgres Lock transactionid 243022 1204 243022 1204 39922.502358 228.822413 1243217 1243213 4 0 90 0 22 0 2025-03-25 11:00:20 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 243020 1204 243020 1204 322868.346002 1758.190499 1142441 1142440 1 0 71 0 21 0 2025-03-25 11:00:21 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 244293 1273 244293 1273 324877.573577 2009.227575 1148465 1148464 1 0 71 0 21 0 2025-03-25 11:00:22 2931033680287349001 UPDATE pgbench_acco postgres postgres Client ClientRead 245652 9377 245652 9377 51188.172394 1818.464415 2140583 2022215 122067 3699 233128 7358 2150 35 2025-03-25 11:00:22 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 245649 1356 245649 1356 326537.380433 1659.806856 1154806 1154805 1 0 71 0 21 0 2025-03-25 11:00:23 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 246817 1168 246817 1168 328234.703307 1697.322874 1160290 1160289 1 0 71 0 21 0 2025-03-25 11:00:24 7073332947325598809 UPDATE pgbench_bran postgres postgres [NULL] [NULL] 247952 1135 247952 1135 329774.702925 1539.999618 1165527 1165526 1 0 71 0 21 0 2025-03-25 11:00:24 5744520630148654507 SELECT abalance FROM postgres postgres [NULL] [NULL] 247954 11679 247954 11679 2917.940273 114.347514 1121749 1121749 0 0 0 0 0 0 2025-03-25 11:00:25 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 249226 1274 249226 1274 331636.450658 1861.747733 1171570 1171569 1 0 71 0 21 0 2025-03-25 11:00:26 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 250306 1080 250306 1080 333492.254318 1855.803660 1176650 1176649 1 0 71 0 21 0 2025-03-25 11:00:27 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 251461 1155 251461 1155 335161.753926 1669.499608 1182023 1182022 1 0 71 0 21 0 2025-03-25 11:00:27 7113545590461720994 SELECT CASE WHEN pg postgres postgres Client ClientRead 3783 1 3783 1 373.398846 0.098783 0 0 0 0 0 0 0 0 2025-03-25 11:00:28 7073332947325598809 UPDATE pgbench_bran postgres postgres Lock transactionid 252639 1178 252639 1178 336785.119418 1623.365492 1187489 1187488 1 0 71 0 21 0 2025-03-25 11:00:29 2931033680287349001 UPDATE pgbench_acco postgres postgres Client ClientRead 254002 8350 254002 8350 52644.978750 1456.806356 2198983 2076916 125384 3317 239263 6135 2180 30 Query Examples Depending on screensize, you might want to set format to aligned, especially when querying pstat_snap_diff_all:\n\\pset format aligned What was happening:\nselect * from pgstat_snap_diff order by 1; What was every query doing:\nselect * from pgstat_snap_diff order by 2,1; Which database touched the most rows:\nselect sum(rows_d),datname from pgstat_snap_diff group by datname; Which query DML affected the most rows:\nselect sum(rows_d),queryid,query from pgstat_snap_diff where upper(query) not like \u0026#39;SELECT%\u0026#39; group by queryid,query; What wait events happened which weren\u0026rsquo;t of type Client:\nselect * from pgstat_snap_diff where wait_event_type is not null and wait_event_type \u0026lt;\u0026gt; \u0026#39;Client\u0026#39; order by 2,1; If needed you can access all columns for a particular query directly in the history tables:\nselect * from pgstat_snap_stat_history where queryid=\u0026#39;123455678909876\u0026#39;; ","description":" Purpose of this script The cumulative statistics system (CSS) in PostgreSQL and pg_stat_statements in particular lack any timing information, all values are cumulative and the only way to figure out the difference between query executions is to reset the stats every time or work with averages.\nWith the pgstat_snap extension, you can create timestamped snapshots of pg_stat_statements and pg_stat_activity when needed. It also provides views that show the difference between every snapshot for every query and database.\n"},{"id":7,"href":"/postgres/","title":"Postgres","parent":"Welcome to my little place on the internet","content":"","description":""},{"id":8,"href":"/stuff/","title":"Stuff","parent":"Welcome to my little place on the internet","content":"","description":""},{"id":9,"href":"/tags/","title":"Tags","parent":"Welcome to my little place on the internet","content":"","description":""}]